<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. Published in the Special Issue of TVCG for ISMAR 2020."/>
    <title>Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: #fdfcf9 no-repeat fixed top left;
        font-family:'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone</h2>
            <h5 style="color:#6e6e6e;">Special Issue of IEEE Transactions on Visualization and Computer Graphics for ISMAR 2020</h5>
            <hr>
            <h6> <a target="_blank">Xingbin Yang</a><sup>1*</sup>, 
                 <a target="_blank">Liyang Zhou</a><sup>1*</sup>, 
				 <a target="_blank">Hanqing Jiang</a><sup>1*</sup>, 
                <a target="_blank">Zhongliang Tang</a><sup>1</sup>,
                <a target="_blank">Yuanbo Wang</a><sup>1</sup>,
				<a href="http://www.cad.zju.edu.cn/bao/" target="_blank">Hujun Bao</a><sup>2</sup>,
                <a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank">Guofeng Zhang</a><sup>2+</sup></h6>
            <p> <sup>1</sup>SenseTime Research&nbsp;&nbsp; 
                <sup>2</sup>State Key Lab of CAD & CG, Zhejiang University
                <br>
                <sup>*</sup> denotes equal contribution and joint first authorship
				<br>
				<sup>+</sup> corresponding author
            </p>

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://ieeexplore.ieee.org/document/9201064" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <img class="img-fluid" src="images/teaser.png" alt="Mobile3DRecon Teaser" width="100%">
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px;">
            <p class="text-justify">
              We present a real-time monocular 3D reconstruction system on a mobile phone, called Mobile3DRecon. Using an embedded monocular camera, our system provides an online mesh generation capability on back end together with real-time 6DoF pose tracking on front end for users to achieve realistic AR effects and interactions on mobile phones. Unlike most existing state-of-the-art systems which produce only point cloud based 3D models online or surface mesh offline, we propose a novel online incremental mesh generation approach to achieve fast online dense surface mesh reconstruction to satisfy the demand of real-time AR applications. For each keyframe of 6DoF tracking, we perform a robust monocular depth estimation, with a multi-view semi-global matching method followed by a depth refinement post-processing. The proposed mesh generation module incrementally fuses each estimated keyframe depth map to an online dense surface mesh, which is useful for achieving realistic AR effects such as occlusions and collisions. We verify our real-time reconstruction results on two mid-range mobile platforms. The experiments with quantitative and qualitative evaluation demonstrate the effectiveness of the proposed monocular 3D reconstruction system, which can handle the occlusions and collisions between virtual objects and real scenes to achieve realistic AR effects.            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- Reconstruction Showcase -->
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Reconstruction showcase</h3>
            <hr style="margin-top:0px">
            <br>
        </div>
      </div>
    </div>
  </section>
  <section>
    <div class="container">
      <div class="row">
        <div class="col text-center">
            <video width="100%" playsinline="" controls autoplay loop="loop" preload="" muted="">
                <source src="videos/sup-1.mp4" type="video/mp4">
            </video>
        </div>
        <div class="col text-center">
            <video width="100%" playsinline="" controls autoplay loop="loop" preload="" muted="">
                <source src="videos/sup-2.mp4" type="video/mp4">
            </video>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- system overview -->
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>System overview</h3>
            <hr style="margin-top:0px">

            <img class="img-fluid" src="images/system_overview.png" alt="Mobile3DRecon System Overview" width="100%">
            <br>
            <br>
            <p class="text-justify"> 
			  Our pipeline performs an incremental online mesh generation. 
			  6DoF poses are tracked with a keyframe-based visual-inertial SLAM system, which
			  maintains a keyframe pool on the back end with a global BA for keyframe pose refinement as feedback to the front end tracking.
			  After the 6DoF tracking is initialized normally on the front end, for a latest incoming keyframe with its globally
			  optimized pose, its dense depth map is online estimated by multi-view SGM, with a part of previous keyframes selected as reference frames.
			  A convolutional neural network follows to refine depth noise. The refined key-frame depth map is then fused to generate dense surface mesh.
			  High level AR applications can utilize this real-time dense mesh and the 6DoF SLAM poses to achieve realistic AR effects on the front end.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- Contributions -->
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Contributions</h3>
            <hr style="margin-top:0px">
        </div>
      </div>
    </div>
  </section>
  <br>
  
  <!--Monocular Depth Estimation-->
  <section>
    <div class="container">
      <div class="row">
        <div class="col text-center">
          <h5 class="text-left"><li><b>Monocular Depth Estimation</b></li></h5>
          <br>
          <img class="img-fluid" src="images/mvs_depth.png" width="70%">
          <p class="text-justify; text-center" style="color:#8899a5; font-size:12px"> 
            Our monocular depth estimation results on two representative keyframes from sequences “Sofa”. (a) The source keyframe image and its two selected reference keyframe images. (b) The depth estimation result of semi-global MVS and the corresponding point cloud by back-projection. (c) The result after confidence-based depth filtering and its corresponding point cloud. (d) The final depth estimation result after DNN-based refinement with its corresponding point cloud.
          </p>
          <img class="img-fluid" src="images/mvs_equation.png" width="40%">
          <p class="text-justify">
            We estimate depth map using an SGM based multi-view stereo approach, which is carried out in a uniformly sampled inverse depth space.
            We use weighted Census Transform to compute patch similarity cost <img class="img-fluid" src="images/census_cost.png" width="5%">, with scores of neighboring frames as weights.
            The aggregated cost <img class="img-fluid" src="images/aggregated_cost.png" width="5%"> among multiple keyframes can be accelerated by NEON, with Winner-Take-All strategy to get the initial depth map. The initial depth map is then refined in sub-level by parabola fitting.
		  <br>
		  </p>
          <img class="img-fluid" src="images/dnn_refinement.png" width="90%">
		  <br>
          <p class="text-justify">
            After the depth filtering, we employ a deep neural network to refine the remaining depth noise. Our network is a two-stage refinement structure.
            The first stage is an image-guided sub-network, which combines filtered depth with the corresponding gray image to reason a coarse refinement result.
			The second stage is a residual U-Net which further refines the previous coarse result to get the final refined depth.
          </p>
       </div>
      </div>
    </div>
  </section>
  
  <!--Incremental Mesh Generation-->
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col text-center">
          <h5 class="text-left"><li><b>Incremental Mesh Generation</b></li></h5>
		  <br>
          <img class="img-fluid" src="images/mesh_result.png" width="80%">
          <p class="text-justify; text-center" style="color:#8899a5; font-size:12px"> 
			Surface mesh generation results of our four experimental sequences (a)"Indoor stairs", (b)"Sofa", (c)"Desktop" and (d)"Cabinet" captured by OPPO R17 Pro. (a) shows some representative keyframes of each sequence. 
		  </p>
		  <p class="text-justify">
			We present a novel incremental mesh generation approach which can update surface mesh in real-time and is more suitable for AR applications on mobile platform with limited computing resources.
		  </p>
          <img class="img-fluid" src="images/incremental_mesh.png" width="30%">
          <p class="text-justify">
			Each estimated depth map is integrated into the TSDF voxels, with associated voxels generated / updated in the conventional way.			
		  </p>
          <img class="img-fluid" src="images/TSDF.png" width="50%">
		  <p class="text-justify">
			To handle the influence of dynamic objects，we project existing voxels to the current frame for depth visibility checking. 
		  </p>
          <img class="img-fluid" src="images/dynamic_object_remove.png" width="100%">
		  <br>
          <p class="text-justify">
			We use an incremental marching cubes algorithm to maintain a status variable for each voxel indicating whether it is newly added or updated or not. For each keyframe, we only need to extract or update mesh triangles from the newly added and updated cubes.
		  </p>
          <img class="img-fluid" src="images/marching_cube.png" width="50%">
		  <br>
        </div>
      </div>
    </div>
  </section>

  <!-- Comparison with state-of-the-art methods -->
  <br><br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with state-of-the-art methods</h3>
            <hr style="margin-top:0px">

            <img class="img-fluid" src="images/compare_SOTA.png" alt="Quantitative Comparison" width="100%">
            <p class="text-justify"> 
            Comparison of the finally fused surface meshes by fusing the estimated depth maps of our Mobile3DRecon and some state-of-the-art methods on sequence “Outdoor stairs” by OPPO R17 Pro. 
			(a) Some representative keyframes. 
            (b) Surface mesh generated by fusing ToF depth maps. 
            (c) <a href="https://github.com/sunghoonim/DPSNet" target="_blank">DPSNet</a>. 
            (d) <a href="https://github.com/sunghoonim/DPSNet" target="_blank">MVDepthNet</a>. 
            (e) Ours Mobile3DRecon.
            </p>

            <img class="img-fluid" src="images/model_accuracy.png" alt="Qualitative Comparison" width="85%">
            <p class="text-justify"> 
            The RMSEs and MAEs of the depth and surface mesh results on our five experimental sequences captured by OPPO R17 Pro with ToF depth measurements as GT. 
			For depth evaluation, only the pixels with valid depths in both GT and the estimated depth map will participate in error calculation. 
			For common depth evaluation, only the pixels with common valid depths in all the methods and GT will participate in evaluation.
			</p>

            <img class="img-fluid" src="images/time_consumption.png" alt="Time Consumption" width="90%">
            <p class="text-justify">
            The detailed per-keyframe time consumptions (in milliseconds) of our Mobile3DRecon in all the substeps. The time statistics are given on two mobile platforms: OPPO R17 Pro with SDM710 and MI8 with SDM845.
			</p>
        </div>
      </div>
    </div>
  </section>
  
  <!-- AR Applications -->
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>AR Applications</h3>
            <hr style="margin-top:0px">

            <img class="img-fluid" src="images/ar_applications.png" alt="Mobile3DRecon System Overview" width="70%">
            <br>
            <br>
            <p class="text-justify"> 
			  AR applications of Mobile3DRecon on mobile platforms: The first row shows the 3D reconstruction and an occlusion effect of an indoor scene on OPPO R17 Pro. The second and third rows illustrate AR occlusion and collision effects of another two scenes on MI8.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@article{yang2020mobile3drecon,
  title={{Mobile3DRecon}: Real-time Monocular 3D Reconstruction on a Mobile Phone},
  author={Yang, Xingbin and Zhou, Liyang, and Jiang, Hanqing and Tang, Zhongliang and Wang, Yuanbo and Bao, Hujun and Zhang, Guofeng},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2020}
}</code></pre>
      </div>
    </div>
  </div>

  <!-- ack -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgements</h3>
          <hr style="margin-top:0px">
          <p class="text-justify">
            We would like to thank Feng Pan and Li Zhou for their kind help in the development of the mobile reconstruction system and the experimental evaluation.
			This work was partially supported by NSF of China (Nos. 61672457 and 61822310), and the Fundamental Research Funds for the Central Universities (No. 2019XZZX004-09).
          </p>
      </div>
    </div>
  </div>
  
  
  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>

  <script type="text/javascript">
    function changePlaybackSpeed(speed)
        {
            document.getElementById('inspect_vid').playbackRate = speed;
        }
        // changePlaybackSpeed(0.25)

    var demo = document.getElementById("header_vid");
    var startTime;
    var timeout = undefined;
    demo.addEventListener("loadstart", function() {
      startTime = Date.now();
      timeout = setTimeout(function () {
        var demoWarning = document.getElementById("demo-warning");
        var giteeLink = document.createElement("a");
        giteeLink.innerText = "mirror hosted in mainland China";
        giteeLink.href = "https://project-pages-1255496016.cos-website.ap-shanghai.myqcloud.com/neuralrecon/";
        // var bilibiliLink = document.createElement("a");
        // var youtubeLink = document.createElement("a");
        // bilibiliLink.innerText = "BiliBili";
        // bilibiliLink.href = "";
        // youtubeLink.innerText = "YouTube";
        // youtubeLink.href = "";

        demoWarning.append("Loading the videos took too long, you can optionally visit this site in the ", giteeLink, ".");
        // demoWarning.append("Loading the video took too long, you can optionally watch it on Bilibili", bilibiliLink, " or YouTube", youtubeLink, ".");
        clearTimeout(timeout);
        timeout = undefined;
      }, 6000);
    });
    demo.addEventListener("loadeddata", function() {
      if (timeout) {
        clearTimeout(timeout);
        timeout = undefined;
      }
    });
//     var source = document.createElement("source");
//     source.setAttribute("src", "/videos/web-scene2.m4v");
//     source.setAttribute("type", "video/webm");
//     demo.appendChild(source);
  </script>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
